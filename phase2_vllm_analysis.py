"""
Phase 2: VLLM Analysis
–ù–µ–∑–∞–≤–∏—Å–∏–º—ã–π –º–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ VLLM
–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: requests, json, pickle, tqdm
"""

import json
import pickle
import requests
import base64
import time
import os
import platform
from pathlib import Path
from typing import List, Dict, Any, Optional
import shutil
from datetime import datetime
from db_manager import ChromaDBManager

# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
FULL_REPROCESS = False  # True = –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å, False = —Ç–æ–ª—å–∫–æ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ

# Try to import tqdm, fallback if not available
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, desc="", leave=True):
        print(f"{desc}...")
        return iterable

# Try to import cv2 for image encoding
try:
    import cv2
    HAS_CV2 = True
except ImportError:
    HAS_CV2 = False
    print("Warning: OpenCV not available - will use pre-encoded frames")


class Phase2VLLMAnalyzer:
    def __init__(self, config_path: str = "vllm_config.json", db_manager: ChromaDBManager = None):
        """Initialize Phase 2 VLLM analyzer"""
        self.config = self.load_config(config_path)
        self.session = requests.Session()
        self.db_manager = db_manager if db_manager else ChromaDBManager()

        # VLLM settings
        self.api_base = self.config["vllm_settings"]["api_base"]
        self.model_name = self.config["vllm_settings"]["model_name"]
        self.temperature = self.config["vllm_settings"]["temperature"]
        self.max_tokens = self.config["vllm_settings"]["max_tokens"]
        self.timeout = self.config["vllm_settings"]["timeout"]

        # –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.current_session_results = {}

        # –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ—Å—Ç–∏
        self.working_dir = Path.cwd()  # –¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è

        # Performance optimization settings
        self.preventive_restart_interval = 40  # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∫–∞–∂–¥—ã–µ N —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        self.processed_segments_count = 0
        self._last_description = None  # –î–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –æ—Ç–≤–µ—Ç–æ–≤
        self.quality_degradation_count = 0

        print(f"‚úì Phase2VLLMAnalyzer initialized")
        print(f"  API: {self.api_base}")
        print(f"  Model: {self.model_name}")
        print(f"  Timeout: {self.timeout}")
        print(f"  Platform: {platform.system()}")
        print(f"  Working dir: {self.working_dir}")
    
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """Load configuration from JSON file"""
        if not Path(config_path).exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def test_vllm_connection(self) -> bool:
        """Test connection to VLLM API"""
        try:
            print("Testing VLLM connection...")
            # Test with models endpoint
            response = self.session.get(f"{self.api_base}/v1/models", timeout=5)
            if response.status_code == 200:
                print("‚úì VLLM API connection successful")
                return True
            else:
                print(f"‚úó VLLM API returned {response.status_code}")
                return False
        except Exception as e:
            print(f"‚úó VLLM API connection failed: {type(e).__name__}")
            return False
    
    def load_phase1_data(self, video_name: str) -> List[Dict]:
        """Load Phase 1 results"""
        pickle_path = Path('output') / f'{video_name}_phase1_data.pkl'
        
        if not pickle_path.exists():
            raise FileNotFoundError(f"Phase 1 data not found: {pickle_path}")
        
        print(f"Loading Phase 1 data from: {pickle_path}")
        with open(pickle_path, 'rb') as f:
            segment_data = pickle.load(f)
        
        print(f"‚úì Loaded {len(segment_data)} segments from Phase 1")
        return segment_data
    
    def create_vllm_prompt(self, previous_context: str, audio_transcript: str) -> str:
        """Create structured prompt for VLLM"""
        
        base_prompt = """Analyze this video segment and provide a detailed description in Russian language ONLY.

PREVIOUS SEGMENT CONTEXT:
{previous_context}

AUDIO TRANSCRIPT:
{audio_transcript}

IMPORTANT: Answer ONLY in Russian language. Do not use any other languages.

TASKS:
1. Describe what's happening in the video: people, actions, environment, objects
2. Translate any speech to Russian if needed
3. Extract keywords (exact terms from the video)
4. Generate search_terms: keywords + synonyms, slang, related concepts for better search
5. Determine content type and atmosphere
6. Note if the topic or character changed compared to the previous segment

Respond with ONLY the JSON object below - no explanations, no additional text, no descriptions.

CRITICAL FORMATTING RULES:
- Return ONLY the JSON structure, nothing else
- Do NOT use line breaks (\\n) or tabs (\\t) inside string values  
- Write all text in single lines without line breaks
- Do NOT add any explanatory text before or after JSON
- Ensure strict JSON syntax

Required JSON format:
{{
  "description": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
  "dialogue_translation": "–ø–µ—Ä–µ–≤–æ–¥ —Ä–µ—á–∏ –Ω–∞ –†–£–°–°–ö–ò–ô —è–∑—ã–∫",
  "keywords": ["—Ç–æ—á–Ω—ã–µ", "–∫–ª—é—á–µ–≤—ã–µ", "—Å–ª–æ–≤–∞", "–∏–∑", "–≤–∏–¥–µ–æ"],
  "search_terms": "–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–ª—é—Å —Å–∏–Ω–æ–Ω–∏–º—ã –∂–∞—Ä–≥–æ–Ω —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –¥–ª—è –ø–æ–∏—Å–∫–∞",
  "content_type": "—Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º",
  "mood_atmosphere": "–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º",
  "scene_change": true/false,
  "new_information": "–Ω–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
  "confidence": "–≤—ã—Å–æ–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–Ω–∏–∑–∫–∞—è"
}}

CRITICAL: 
- Use ONLY Russian language in all fields. No ANY another languages.
- Return ONLY the JSON object above, nothing more.
- Do NOT explain what you're doing.
- Do NOT add any text before or after the JSON."""
        
        return base_prompt.format(
            previous_context=previous_context or "–ù–µ—Ç (—ç—Ç–æ –ø–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç)",
            audio_transcript=audio_transcript or "–ù–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"
        )
    
    def detect_quality_degradation(self, response: str, analysis_json: dict) -> tuple:
        """–î–µ—Ç–µ–∫—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ VLLM

        Returns:
            tuple: (is_degraded: bool, degradation_flags: list, is_critical_loop: bool)
        """

        degradation_flags = []
        critical_loop_flags = ["keyword_loop", "keyword_flood", "search_terms_loop"]

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –ø–æ–ª—è—Ö
        chinese_chars = any('\u4e00' <= char <= '\u9fff' for char in response)
        if chinese_chars:
            degradation_flags.append("chinese_characters")
            print("  ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã - –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞")

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
        description = analysis_json.get('description', '')
        if len(description) < 20:
            degradation_flags.append("short_description")
            print(f"  ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ ({len(description)} —Å–∏–º–≤–æ–ª–æ–≤) - –≤–æ–∑–º–æ–∂–Ω–∞—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è")

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –æ—Ç–≤–µ—Ç—ã
        if self._last_description and self._last_description == description:
            degradation_flags.append("duplicate_description")
            print("  ‚ö†Ô∏è –ü–æ–≤—Ç–æ—Ä—è—é—â–µ–µ—Å—è –æ–ø–∏—Å–∞–Ω–∏–µ - –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞")

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_fields = ['description', 'confidence']
        empty_fields = [f for f in required_fields if not analysis_json.get(f, '').strip()]
        if empty_fields:
            degradation_flags.append("empty_required_fields")
            print(f"  ‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {empty_fields}")

        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è confidence
        confidence = analysis_json.get('confidence', '').lower()
        valid_confidence = ['–≤—ã—Å–æ–∫–∞—è', '—Å—Ä–µ–¥–Ω—è—è', '–Ω–∏–∑–∫–∞—è', 'high', 'medium', 'low']
        if confidence and confidence not in valid_confidence:
            degradation_flags.append("invalid_confidence")
            print(f"  ‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ confidence: {confidence}")

        # 6. –î–µ—Ç–µ–∫—Ü–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤ –≤ keywords (LLM loop) - –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø
        keywords = analysis_json.get('keywords', [])
        if isinstance(keywords, list) and len(keywords) > 5:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–ª–æ–≤–æ –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è >3 —Ä–∞–∑ –ø–æ–¥—Ä—è–¥
            for i in range(len(keywords) - 3):
                if keywords[i] == keywords[i+1] == keywords[i+2] == keywords[i+3]:
                    degradation_flags.append("keyword_loop")
                    print(f"  üî¥ CRITICAL: LLM loop detected: '{keywords[i]}' –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è 4+ —Ä–∞–∑ –ø–æ–¥—Ä—è–¥")
                    break
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö vs –≤—Å–µ–≥–æ
            if len(keywords) > 10:
                unique_ratio = len(set(keywords)) / len(keywords)
                if unique_ratio < 0.3:  # –ú–µ–Ω–µ–µ 30% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
                    degradation_flags.append("keyword_flood")
                    print(f"  üî¥ CRITICAL: Keyword flood: {len(set(keywords))}/{len(keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ({unique_ratio:.1%})")

        # 7. –î–µ—Ç–µ–∫—Ü–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤ –≤ search_terms - –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø
        search_terms = analysis_json.get('search_terms', '')
        if isinstance(search_terms, str) and len(search_terms) > 100:
            words = search_terms.split()
            if len(words) > 10:
                unique_ratio = len(set(words)) / len(words)
                if unique_ratio < 0.3:
                    degradation_flags.append("search_terms_loop")
                    print(f"  üî¥ CRITICAL: Search terms loop: {len(set(words))}/{len(words)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ({unique_ratio:.1%})")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        self._last_description = description

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ loop —Ñ–ª–∞–≥–∏
        is_critical_loop = any(flag in critical_loop_flags for flag in degradation_flags)

        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
        if degradation_flags:
            self.quality_degradation_count += 1
            print(f"  üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞: {', '.join(degradation_flags)}")
            print(f"  üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–π: {self.quality_degradation_count}")
            if is_critical_loop:
                print(f"  üî¥ –¢–†–ï–ë–£–ï–¢–°–Ø –ù–ï–ú–ï–î–õ–ï–ù–ù–´–ô –ü–ï–†–ï–ó–ê–ü–£–°–ö LLM!")
            return (True, degradation_flags, is_critical_loop)

        return (False, [], False)
    
    def check_preventive_restart(self, segment_index: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞"""
        
        # –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∫–∞–∂–¥—ã–µ N —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        if (segment_index + 1) % self.preventive_restart_interval == 0:
            print(f"  üîÑ –í—Ä–µ–º—è –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –ø–æ—Å–ª–µ {segment_index + 1} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            return True
        
        # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–π –∫–∞—á–µ—Å—Ç–≤–∞
        if self.quality_degradation_count >= 3:
            print(f"  üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∏–∑-–∑–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–π ({self.quality_degradation_count})")
            self.quality_degradation_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
            return True
        
        return False
    
    def _extract_from_markdown_blocks(self, response: str) -> str:
        """Extract JSON from markdown code blocks"""
        # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ markdown –æ–±–µ—Ä—Ç–æ–∫
        if '```json' in response:
            # –ù–∞–π—Ç–∏ –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü JSON –±–ª–æ–∫–∞
            start_marker = '```json'
            end_marker = '```'
            
            start_idx = response.find(start_marker)
            if start_idx != -1:
                # –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ ```json
                json_start = start_idx + len(start_marker)
                
                # –ò—â–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π ```
                end_idx = response.find(end_marker, json_start)
                if end_idx != -1:
                    return response[json_start:end_idx].strip()
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–≥–æ - –±–µ—Ä–µ–º –≤—Å–µ –ø–æ—Å–ª–µ ```json
                    return response[json_start:].strip()
        
        # –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å ```
        elif response.startswith('```') and response.endswith('```'):
            lines = response.split('\n')
            if len(lines) > 2:
                return '\n'.join(lines[1:-1])
        
        # Return as-is if no markdown wrapping
        return response
    
    def _is_structured_markdown(self, text: str) -> bool:
        """Check if text looks like structured markdown with our expected fields"""
        # –ü–æ–∏—Å–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ markdown –æ—Ç–≤–µ—Ç–∞
        markdown_indicators = [
            '**–û–ø–∏—Å–∞–Ω–∏–µ:**', '**Description:**',
            '**–î–∏–∞–ª–æ–≥:**', '**Dialogue:**', 
            '**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:**', '**Keywords:**',
            '**–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞:**', '**Content type:**',
            '**–ê—Ç–º–æ—Åfera:**', '**Mood:**',
            '- –û–ø–∏—Å–∞–Ω–∏–µ:', '- Description:',
            '- –î–∏–∞–ª–æ–≥:', '- Dialogue:',
            '- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:', '- Keywords:'
        ]
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞, —Å—á–∏—Ç–∞–µ–º structured
        found_indicators = sum(1 for indicator in markdown_indicators if indicator.lower() in text.lower())
        return found_indicators >= 2
    
    def _convert_markdown_to_json(self, markdown: str) -> str:
        """Convert structured markdown to JSON"""
        import re
        
        try:
            # –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π JSON –æ–±—ä–µ–∫—Ç
            result = {}
            
            # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–ª–µ–π
            patterns = {
                'description': [
                    r'\*\*–û–ø–∏—Å–∞–Ω–∏–µ:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'\*\*Description:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'- –û–ø–∏—Å–∞–Ω–∏–µ:\s*(.+?)(?=\n-|\n\n|$)',
                    r'- Description:\s*(.+?)(?=\n-|\n\n|$)'
                ],
                'dialogue_translation': [
                    r'\*\*–î–∏–∞–ª–æ–≥:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'\*\*Dialogue:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'- –î–∏–∞–ª–æ–≥:\s*(.+?)(?=\n-|\n\n|$)',
                    r'- Dialogue:\s*(.+?)(?=\n-|\n\n|$)'
                ],
                'keywords': [
                    r'\*\*–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'\*\*Keywords:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:\s*(.+?)(?=\n-|\n\n|$)',
                    r'- Keywords:\s*(.+?)(?=\n-|\n\n|$)'
                ],
                'content_type': [
                    r'\*\*–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'\*\*Content type:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'- –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞:\s*(.+?)(?=\n-|\n\n|$)',
                    r'- Content type:\s*(.+?)(?=\n-|\n\n|$)'
                ],
                'mood_atmosphere': [
                    r'\*\*–ê—Ç–º–æ—Å—Ñ–µ—Ä–∞:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'\*\*Mood:\*\*\s*(.+?)(?=\*\*|\n\n|$)',
                    r'- –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞:\s*(.+?)(?=\n-|\n\n|$)',
                    r'- Mood:\s*(.+?)(?=\n-|\n\n|$)'
                ]
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ
            for field, field_patterns in patterns.items():
                for pattern in field_patterns:
                    match = re.search(pattern, markdown, re.DOTALL | re.IGNORECASE)
                    if match:
                        value = match.group(1).strip()
                        
                        # –î–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ —Å–ø–∏—Å–æ–∫
                        if field == 'keywords' and value:
                            # –£–¥–∞–ª—è–µ–º markdown —Å–ø–∏—Å–∫–∏ –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º
                            value = re.sub(r'^[-*]\s*', '', value, flags=re.MULTILINE)
                            keywords = [kw.strip() for kw in re.split(r'[,;]|\n', value) if kw.strip()]
                            if keywords:
                                result[field] = keywords
                            else:
                                result[field] = [value]
                        else:
                            result[field] = value
                        break
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –µ—Å–ª–∏ –ø–æ–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
            if 'description' not in result:
                result['description'] = '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ'
            if 'confidence' not in result:
                result['confidence'] = '—Å—Ä–µ–¥–Ω—è—è'
            if 'scene_change' not in result:
                result['scene_change'] = False
            if 'new_information' not in result:
                result['new_information'] = '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ markdown'
                
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON
            import json
            json_str = json.dumps(result, ensure_ascii=False, indent=2)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª—Å—è –≤–∞–ª–∏–¥–Ω—ã–π JSON
            json.loads(json_str)
            return json_str
            
        except Exception as e:
            print(f"    ‚ùå Error converting markdown to JSON: {e}")
            return None
    
    def extract_json_from_response(self, response: str) -> str:
        """Extract JSON from markdown code block or convert markdown structure to JSON"""
        import re
        import json
        
        response = response.strip()
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –≤ markdown –±–ª–æ–∫–∞—Ö
        extracted = self._extract_from_markdown_blocks(response)
        
        # 2. –ï—Å–ª–∏ –Ω–∞—à–ª–∏ JSON –≤ –±–ª–æ–∫–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if extracted != response:
            print(f"    üìù Extracted from markdown code block")
        
        # 3. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ JSON, –Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π markdown - –ø—ã—Ç–∞–µ–º—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
        elif self._is_structured_markdown(response):
            print(f"    üîÑ Detected structured markdown, attempting conversion...")
            converted = self._convert_markdown_to_json(response)
            if converted:
                print(f"    ‚úÖ Successfully converted markdown to JSON")
                extracted = converted
            else:
                print(f"    ‚ùå Failed to convert markdown to JSON")
                extracted = response
        else:
            # Return as-is if no special handling needed
            extracted = response
        
        # –ê–ö–ö–£–†–ê–¢–ù–ê–Ø –û–ß–ò–°–¢–ö–ê JSON
        
        # 1. –£–¥–∞–ª–∏—Ç—å —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        # –ù–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –∏ —Ç–∞–±—ã –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON
        cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', extracted)
        
        # 2. –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        lines = cleaned.split('\n')
        cleaned_lines = [line.rstrip() for line in lines]
        cleaned = '\n'.join(cleaned_lines)
        
        # 3. –£–¥–∞–ª–∏—Ç—å trailing commas –µ—Å–ª–∏ –µ—Å—Ç—å
        cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON
        cleaned = cleaned.strip()
        
        # –ï—Å–ª–∏ –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        if not cleaned:
            print(f"    ‚ö†Ô∏è JSON empty after basic cleaning")
            return '{}'
        
        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
        try:
            json.loads(cleaned)
            print(f"    ‚úì JSON is valid after cleaning")
            return cleaned
        except json.JSONDecodeError as e:
            print(f"    JSON still invalid after cleaning: {e}")
            print(f"    Cleaned length: {len(cleaned)}")
            print(f"    First 300 chars: {repr(cleaned[:300])}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ—á–∏–Ω–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
            if cleaned.startswith('{') and not cleaned.rstrip().endswith('}'):
                fixed = cleaned.rstrip() + '}'
                try:
                    json.loads(fixed)
                    print(f"    ‚úì Fixed by adding closing brace")
                    return fixed
                except:
                    print(f"    ‚ùå Still broken after adding closing brace")
            
            # –ï—Å–ª–∏ –≤—Å—ë —Å–ª–æ–º–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∫–∞–∫ –µ—Å—Ç—å
            # –ù–ï –ø–æ–¥–º–µ–Ω—è–µ–º –Ω–∞ –∑–∞–≥–ª—É—à–∫—É - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á—Ç–æ –≤–µ—Ä–Ω—É–ª LLM
            print(f"    ‚Üí Returning original LLM response as fallback")
            return cleaned
    
    def get_base64_frames(self, data: Dict) -> List[str]:
        """Get base64 frames from segment data"""
        # Try to use pre-encoded base64 frames first
        if 'frames_base64' in data and data['frames_base64']:
            return data['frames_base64']
        
        # Fallback to encoding frames if cv2 is available
        if HAS_CV2 and 'frames' in data:
            frames = data['frames']
            base64_images = []
            for frame in tqdm(frames, desc="    Converting frames to base64", leave=False):
                success, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
                if success:
                    base64_string = base64.b64encode(buffer).decode('utf-8')
                    base64_images.append(base64_string)
            return base64_images
        
        # No frames available
        return []
    
    def analyze_segment_vllm(self, base64_frames: List[str], prompt: str) -> Optional[str]:
        """Analyze segment frames with VLLM"""
        
        if not base64_frames:
            return None
        
        # Prepare VLLM request
        messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "text", "text": prompt}
                ] + [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{img}"}
                    } for img in base64_frames
                ]
            }
        ]
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        # Make request with connection error handling
        try:
            response = self.session.post(
                f"{self.api_base}/v1/chat/completions",
                json=payload,
                timeout=self.timeout
            )
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
            print(f"    ‚ùå VLLM connection error: {type(e).__name__}")
            return None
        except Exception as e:
            print(f"    ‚ùå VLLM request error: {type(e).__name__}: {e}")
            return None
        
        if response.status_code == 200:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                content = result["choices"][0]["message"]["content"]
                if content:
                    # Clean up response: trim whitespace and newlines
                    cleaned_content = content.strip()
                    if cleaned_content:
                        return cleaned_content
                    else:
                        print(f"    ‚ö†Ô∏è VLLM returned content but it's empty after cleaning")
                else:
                    print(f"    ‚ö†Ô∏è VLLM returned null/empty content")
            else:
                print(f"    ‚ö†Ô∏è VLLM response missing choices: {result}")
        else:
            print(f"    ‚ùå VLLM HTTP error {response.status_code}: {response.text}")
        
        return None
    
    def get_flag_path(self, filename: str) -> Path:
        """–ü–æ–ª—É—á–∏—Ç—å –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ñ–ª–∞–≥–∞"""
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        return self.working_dir / filename
    
    def write_flag_file(self, filename: str, data: Dict[str, Any]) -> bool:
        """–ö—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞ —Ñ–ª–∞–≥–∞"""
        flag_path = self.get_flag_path(filename)
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows –∏ Linux
            with open(flag_path, 'w', encoding='utf-8', newline='\n') as f:
                json.dump(data, f, ensure_ascii=True, indent=2)
            
            print(f"‚úì Flag file created: {flag_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating flag file {flag_path}: {e}")
            return False
    
    def read_status_file(self, filename: str) -> Optional[Dict[str, Any]]:
        """–ö—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å—Ç–∞—Ç—É—Å–∞"""
        status_path = self.get_flag_path(filename)
        
        if not status_path.exists():
            return None
        
        try:
            with open(status_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå Error reading status file {status_path}: {e}")
            return None
    
    def stop_model(self, reason: str = "manual") -> None:
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        print(f"üõë –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–∏ —É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞ (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
        
        flag_data = {
            "action": "stop",
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "reason": reason,
            "platform": platform.system()
        }
        
        if self.write_flag_file('restart_vllm.flag', flag_data):
            print(f"‚úÖ –§–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å–æ–∑–¥–∞–Ω, –æ–∂–∏–¥–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
    
    def start_model(self, model_script: str = "qwen2_5_vl_32b.sh", reason: str = "manual") -> None:
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥"""
        flag_data = {
            "action": "restart",
            "model_script": model_script,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "reason": reason,
            "platform": platform.system()
        }
        
        if self.write_flag_file('restart_vllm.flag', flag_data):
            print(f"üöÄ –ó–∞–ø—Ä–æ—à–µ–Ω –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏: {model_script} (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–ª–∞–≥–∞ –∑–∞–ø—É—Å–∫–∞")
    
    def restart_model(self, model_script: str = "qwen3_vl_32b.sh", reason: str = "error") -> None:
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        print(f"üîÑ –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ —É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞: {model_script} (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
        
        flag_data = {
            "action": "restart",
            "model_script": model_script,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "reason": reason,
            "platform": platform.system()
        }
        
        if self.write_flag_file('restart_vllm.flag', flag_data):
            print(f"‚úÖ –§–ª–∞–≥ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ —Å–æ–∑–¥–∞–Ω, –æ–∂–∏–¥–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–ª–∞–≥–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
    
    def get_model_status(self) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏ –æ—Ç –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        return self.read_status_file('model_status.json')
    
    def check_model_health(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–¥–æ—Ä–æ–≤—å–µ –º–æ–¥–µ–ª–∏ (–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API)"""
        try:
            return self.test_vllm_connection()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def auto_start_model_if_needed(self, max_wait_time: int = 180) -> bool:
        """–ê–≤—Ç–æ–∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"""
        print("üöÄ –ê–≤—Ç–æ–∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏...")
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä (—Ç–æ–ª—å–∫–æ –Ω–∞ Linux/WSL)
        if platform.system() == "Linux":
            print("   –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä...")
            try:
                import subprocess
                result = subprocess.run(['./model_orchestrator_v2.sh', 'cron'], 
                                      capture_output=True, text=True, timeout=60,
                                      cwd=str(self.working_dir))
                if result.returncode == 0:
                    print("   ‚úÖ –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –∑–∞–ø—É—â–µ–Ω")
                else:
                    print(f"   ‚ö†Ô∏è –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∫–æ–¥–æ–º {result.returncode}")
                    print(f"   stderr: {result.stderr[:200] if result.stderr else '–Ω–µ—Ç –æ—à–∏–±–æ–∫'}")
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞: {e}")
        else:
            print("   Windows detected - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–ª–∞–≥–æ–≤—ã–π –∑–∞–ø—É—Å–∫...")
        
        # 2. –í –ª—é–±–æ–º —Å–ª—É—á–∞–µ —Å–æ–∑–¥–∞–µ–º —Ñ–ª–∞–≥ –∑–∞–ø—É—Å–∫–∞ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)
        print("   –°–æ–∑–¥–∞–µ–º —Ñ–ª–∞–≥ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏...")
        self.start_model("qwen2_5_vl_32b.sh", "auto_start_python")
        
        # 3. –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏
        print(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏ (–º–∞–∫—Å–∏–º—É–º {max_wait_time} —Å–µ–∫—É–Ω–¥)...")
        
        for attempt in range(max_wait_time // 10):
            time.sleep(10)
            if self.test_vllm_connection():
                print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–ø—É—Å—Ç–∏–ª–∞—Å—å –ø–æ—Å–ª–µ {(attempt + 1) * 10} —Å–µ–∫—É–Ω–¥")
                return True
            else:
                print(f"   ‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_wait_time // 10}: –º–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤–∞...")
        
        print(f"   ‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª–∞—Å—å –∑–∞ {max_wait_time} —Å–µ–∫—É–Ω–¥")
        return False
    
    def wait_for_model_stop(self, timeout: int = 60) -> bool:
        """–ñ–¥–∞—Ç—å –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏"""
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ (–º–∞–∫—Å–∏–º—É–º {timeout} —Å–µ–∫—É–Ω–¥)...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            status = self.get_model_status()
            if not status or status.get('status') == 'stopped':
                print("‚úÖ –ú–æ–¥–µ–ª—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                return True
            time.sleep(2)
        
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∞—Å—å –≤ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è")
        return False
    
    def request_model_restart(self, model_script: str = "qwen2_5_vl_32b.sh", reason: str = "error") -> None:
        """–ó–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        self.restart_model(model_script, reason)
    
    def wait_for_model_restart(self, timeout: int = 180) -> bool:
        """–ñ–¥–∞—Ç—å –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å"""
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏ (–º–∞–∫—Å–∏–º—É–º {timeout} —Å–µ–∫—É–Ω–¥)...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ñ–ª–∞–≥ –∏—Å—á–µ–∑ (–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–ª)
            flag_file = self.get_flag_path('restart_vllm.flag')
            if not flag_file.exists():
                print("‚úì –§–ª–∞–≥ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º")
                break
            time.sleep(5)
        
        # –î–∞—Ç—å –≤—Ä–µ–º—è –Ω–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫
        print("‚è≥ –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫...")
        time.sleep(45)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å API
        max_checks = 18  # 90 —Å–µ–∫—É–Ω–¥
        for i in range(max_checks):
            if self.test_vllm_connection():
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞!")
                return True
            else:
                print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ {i+1}/{max_checks}: –º–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤–∞...")
                time.sleep(5)
        
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
        return False

    def get_previous_segment_context(self, video_name: str, current_index: int) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if current_index == 0:
            return ""  # –ü–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç - –Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        previous_index = current_index - 1
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
        if previous_index in self.current_session_results:
            cached_result = self.current_session_results[previous_index]
            if cached_result.get('success', False):
                try:
                    analysis = json.loads(cached_result.get('analysis', '{}'))
                    description = analysis.get('description', '')
                    if description:
                        print(f"  ‚úì –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞ {previous_index + 1} (—Ç–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è): {description[:100]}...")
                        return description
                except:
                    pass
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ, –∏—â–µ–º –≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        existing = self.load_existing_results(video_name)
        if existing:
            for segment in existing.get('segments', []):
                if segment.get('segment_index') == previous_index:
                    # –ï—Å–ª–∏ –µ—Å—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç
                    if segment.get('success', False) or ('parse_error' not in segment and 'error' not in segment):
                        try:
                            analysis = json.loads(segment.get('analysis', '{}'))
                            description = analysis.get('description', '')
                            if description:
                                print(f"  ‚úì –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞ {previous_index + 1} (—Ñ–∞–π–ª): {description[:100]}...")
                                return description
                        except:
                            pass
        
        print(f"  ‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ {previous_index + 1} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return ""

    def process_phase2_analysis(self, video_name: str) -> None:
        """Process Phase 2: VLLM Analysis"""
        print(f"\n=== PHASE 2: VLLM Analysis for {video_name} ===")

        # === Check ChromaDB status ===
        video_status = self.db_manager.get_video_status(video_name)

        if not video_status:
            print(f"‚ùå Video {video_name} not found in ChromaDB - run Phase 1 first!")
            return

        if video_status.get("phase1_status") != "completed":
            print(f"‚ö†Ô∏è Phase 1 not completed for {video_name}, skipping...")
            return

        if video_status.get("phase2_status") == "completed":
            print(f"‚úì Phase 2 already completed for {video_name}, skipping...")
            return

        if video_status.get("phase2_status") == "processing":
            # Check timeout
            start_time = datetime.fromisoformat(video_status["phase2_start_time"])
            video_duration = video_status["video_duration"]
            timeout = video_duration * 2

            if (datetime.now() - start_time).total_seconds() < timeout:
                print(f"‚ö†Ô∏è Phase 2 still processing for {video_name}, skipping...")
                return
            else:
                print(f"üîÑ Phase 2 timeout detected for {video_name}, reprocessing...")

        # Set status to "processing"
        self.db_manager.update_video_status(
            video_name=video_name,
            phase2_status="processing",
            phase2_start_time=datetime.now().isoformat()
        )

        # Test VLLM connection and auto-start if needed
        if not self.test_vllm_connection():
            print("‚ö†Ô∏è VLLM API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∑–∞–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏...")
            self.auto_start_model_if_needed()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑ –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞
            if not self.test_vllm_connection():
                raise ConnectionError("VLLM API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–∞–∂–µ –ø–æ—Å–ª–µ –∞–≤—Ç–æ–∑–∞–ø—É—Å–∫–∞")
        
        # Load Phase 1 data
        segment_data = self.load_phase1_data(video_name)
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
        segments_to_process = self.get_segments_to_process(video_name, len(segment_data))
        
        if not segments_to_process:
            print("‚úÖ –í—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã —É–∂–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
            return
        
        print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(segments_to_process)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        results = []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º segment_data –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
        segments_to_analyze = [segment_data[i] for i in segments_to_process]
        
        for data in tqdm(segments_to_analyze, desc="Phase 2: VLLM Analysis"):
            i = data['index']
            segment = data['segment']
            transcript = data['transcript']
            
            print(f"\nAnalyzing segment {i+1}/60 with VLLM")
            print(f"  Time: {segment['start']:.1f}s - {segment['end']:.1f}s")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
            if self.check_preventive_restart(i):
                print(f"  üîÑ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ VLLM...")
                self.restart_model(reason=f"preventive_restart_segment_{i+1}")
                
                # –û–∂–∏–¥–∞–Ω–∏–µ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                print(f"  ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ VLLM (180 —Å–µ–∫—É–Ω–¥)...")
                time.sleep(180)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                if not self.check_model_health():
                    print(f"  ‚ùå VLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
                    print(f"  üõë –û–°–¢–ê–ù–û–í–ö–ê: –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ VLLM")
                    break
                
                print(f"  ‚úÖ VLLM –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
            
            # Get base64 frames
            base64_frames = self.get_base64_frames(data)
            if not base64_frames:
                print(f"  ‚ùå No frames available for segment {i+1}")
                print(f"  üõë –û–°–¢–ê–ù–û–í–ö–ê: –Ω–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä—ã")
                break
            
            print(f"  ‚úì Got {len(base64_frames)} base64 frames")
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            previous_description = self.get_previous_segment_context(video_name, i)
            
            # Create prompt
            prompt = self.create_vllm_prompt(previous_description, transcript)
            
            # Analyze with VLLM (with retry logic)
            analysis = None
            max_retries = 5
            
            for attempt in range(max_retries):
                print(f"  VLLM attempt {attempt+1}/{max_retries}...")
                analysis = self.analyze_segment_vllm(base64_frames, prompt)
                
                if analysis and analysis.strip():
                    print(f"  ‚úì VLLM analysis completed")
                    # Debug: show what VLLM returned
                    print(f"  Raw VLLM response: {analysis}")
                    break
                else:
                    print(f"  ‚ö†Ô∏è Empty/invalid VLLM response on attempt {attempt+1}")
                    if attempt < max_retries - 1:
                        print(f"    Retrying...")
                        time.sleep(1.0)  # Brief pause before retry
                    else:
                        print(f"  üõë –û–°–¢–ê–ù–û–í–ö–ê: VLLM –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                        print(f"     –í–æ–∑–º–æ–∂–Ω–æ VLLM —Å–µ—Ä–≤–µ—Ä –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å")
                        break
            
            if not analysis or not analysis.strip():
                print(f"  ‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç VLLM –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {i+1}")
                print(f"     –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏...")
                
                # –ó–∞–ø—Ä–æ—Å–∏—Ç—å –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
                self.restart_model(reason=f"empty_response_segment_{i+1}")
                
                # –î–æ–∂–¥–∞—Ç—å—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                if self.wait_for_model_restart():
                    print(f"  üîÑ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–∞, –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å–µ–≥–º–µ–Ω—Ç {i+1}")
                    # –ü–æ–≤—Ç–æ—Ä–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —ç—Ç–æ–≥–æ –∂–µ —Å–µ–≥–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                    analysis = self.analyze_segment_vllm(base64_frames, prompt)
                    if analysis and analysis.strip():
                        print(f"  ‚úÖ –ü–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {i+1}")
                    else:
                        print(f"  üõë –ü–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –≤—Å–µ –µ—â–µ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º")
                        break
                else:
                    print(f"  üõë –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
                    break
                
            if analysis and analysis.strip():
                try:
                    # Extract and validate JSON - –º–µ—Ç–æ–¥ —É–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é
                    clean_json = self.extract_json_from_response(analysis)
                    print(f"  ‚úì JSON cleaned and validated")
                    
                    # –ü–∞—Ä—Å–∏–º —É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–π –∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON
                    analysis_json = json.loads(clean_json)
                    
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –≤ –∏–º–µ–Ω–∞—Ö –ø–æ–ª–µ–π
                    if 'mchange' in analysis_json and 'scene_change' not in analysis_json:
                        analysis_json['scene_change'] = analysis_json['mchange']
                        del analysis_json['mchange']
                    
                    if 'information' in analysis_json and 'new_information' not in analysis_json:
                        analysis_json['new_information'] = analysis_json['information'] 
                        del analysis_json['information']
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (–º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
                    required_fields = ['description', 'confidence']
                    missing_fields = [f for f in required_fields if f not in analysis_json]
                    if missing_fields:
                        print(f"  ‚ö†Ô∏è Missing fields: {missing_fields} - using defaults")
                        if 'description' not in analysis_json:
                            analysis_json['description'] = "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
                        if 'confidence' not in analysis_json:
                            analysis_json['confidence'] = "—Å—Ä–µ–¥–Ω—è—è"
                    
                    # –î–µ—Ç–µ–∫—Ü–∏—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ VLLM
                    quality_degraded, degradation_flags, is_critical_loop = self.detect_quality_degradation(analysis, analysis_json)
                    if quality_degraded:
                        print(f"  ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {i+1}")

                        # –ü—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º loop - –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∏ retry
                        if is_critical_loop:
                            print(f"  üî¥ CRITICAL LOOP DETECTED! –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ LLM –∏ –ø–æ–≤—Ç–æ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞...")
                            self.restart_model(reason=f"critical_loop_segment_{i+1}")
                            if self.wait_for_model_restart():
                                print(f"  üîÑ LLM –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–∞, –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å–µ–≥–º–µ–Ω—Ç {i+1}")
                                # –ü–æ–≤—Ç–æ—Ä–∏—Ç—å —ç—Ç–æ—Ç –∂–µ —Å–µ–≥–º–µ–Ω—Ç
                                retry_analysis = self.analyze_segment_vllm(base64_frames, prompt)
                                if retry_analysis and retry_analysis.strip():
                                    retry_clean_json = self.extract_json_from_response(retry_analysis)
                                    try:
                                        retry_analysis_json = json.loads(retry_clean_json)
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º retry —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–º–µ—Å—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ
                                        analysis_json = retry_analysis_json
                                        clean_json = retry_clean_json
                                        print(f"  ‚úÖ –°–µ–≥–º–µ–Ω—Ç {i+1} —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ LLM")
                                    except json.JSONDecodeError:
                                        print(f"  ‚ö†Ô∏è JSON –æ—à–∏–±–∫–∞ –ø–æ—Å–ª–µ retry, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª")
                            else:
                                print(f"  ‚ö†Ô∏è –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –µ—Å—Ç—å")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ keywords —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                    if 'keywords' in analysis_json and not isinstance(analysis_json['keywords'], list):
                        if isinstance(analysis_json['keywords'], str):
                            analysis_json['keywords'] = [analysis_json['keywords']]
                        else:
                            analysis_json['keywords'] = []
                    
                    previous_description = analysis_json.get('description', '')
                    
                    result = {
                        'segment_index': i,
                        'start_time': segment['start'],
                        'end_time': segment['end'],
                        'duration': segment['duration'],
                        'transcript': transcript,
                        'analysis': clean_json,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π JSON –≤–º–µ—Å—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ
                        'frame_count': len(base64_frames),
                        'success': True
                    }
                    results.append(result)
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    self.current_session_results[i] = result
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    self.processed_segments_count += 1
                    
                    print(f"  ‚úÖ Segment {i+1} processed successfully")
                    print(f"  üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {self.processed_segments_count}")
                    
                except json.JSONDecodeError as e:
                    print(f"  ‚ùå JSON parsing error for segment {i+1}:")
                    print(f"      Error: {e}")
                    print(f"      Raw VLLM response length: {len(analysis)} chars")
                    print(f"      First 200 chars of raw response: {repr(analysis[:200])}")
                    print(f"      Cleaned JSON length: {len(clean_json)} chars")  
                    print(f"      First 200 chars of cleaned JSON: {repr(clean_json[:200])}")
                    
                    # –ù–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–≤—Ç–æ—Ä–∞
                    max_restart_attempts = 3
                    restart_successful = False
                    
                    for restart_attempt in range(max_restart_attempts):
                        print(f"  üîÑ –ü–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ {restart_attempt+1}/{max_restart_attempts} –∏–∑-–∑–∞ JSON –æ—à–∏–±–∫–∏...")
                        self.restart_model(reason=f"json_parse_error_segment_{i+1}_attempt_{restart_attempt+1}")
                        
                        # –î–æ–∂–¥–∞—Ç—å—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                        if self.wait_for_model_restart():
                            print(f"  üîÑ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–∞, –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å–µ–≥–º–µ–Ω—Ç {i+1}")
                            # –ü–æ–≤—Ç–æ—Ä–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —ç—Ç–æ–≥–æ –∂–µ —Å–µ–≥–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                            retry_analysis = self.analyze_segment_vllm(base64_frames, prompt)
                            if retry_analysis and retry_analysis.strip():
                                try:
                                    # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                                    retry_clean_json = self.extract_json_from_response(retry_analysis)
                                    retry_analysis_json = json.loads(retry_clean_json)
                                    
                                    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –≤—ã—à–µ
                                    if 'mchange' in retry_analysis_json and 'scene_change' not in retry_analysis_json:
                                        retry_analysis_json['scene_change'] = retry_analysis_json['mchange']
                                        del retry_analysis_json['mchange']
                                    
                                    if 'information' in retry_analysis_json and 'new_information' not in retry_analysis_json:
                                        retry_analysis_json['new_information'] = retry_analysis_json['information'] 
                                        del retry_analysis_json['information']
                                    
                                    required_fields = ['description', 'confidence']
                                    missing_fields = [f for f in required_fields if f not in retry_analysis_json]
                                    if missing_fields:
                                        if 'description' not in retry_analysis_json:
                                            retry_analysis_json['description'] = "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
                                        if 'confidence' not in retry_analysis_json:
                                            retry_analysis_json['confidence'] = "—Å—Ä–µ–¥–Ω—è—è"
                                    
                                    if 'keywords' in retry_analysis_json and not isinstance(retry_analysis_json['keywords'], list):
                                        if isinstance(retry_analysis_json['keywords'], str):
                                            retry_analysis_json['keywords'] = [retry_analysis_json['keywords']]
                                        else:
                                            retry_analysis_json['keywords'] = []
                                    
                                    result = {
                                        'segment_index': i,
                                        'start_time': segment['start'],
                                        'end_time': segment['end'],
                                        'duration': segment['duration'],
                                        'transcript': transcript,
                                        'analysis': retry_clean_json,
                                        'frame_count': len(base64_frames),
                                        'success': True,
                                        'retry_after_restart': True,
                                        'restart_attempts': restart_attempt + 1
                                    }
                                    results.append(result)
                                    self.current_session_results[i] = result
                                    print(f"  ‚úÖ Segment {i+1} processed successfully after restart (attempt {restart_attempt+1})")
                                    restart_successful = True
                                    break  # –£—Å–ø–µ—à–Ω–æ - –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–ø—ã—Ç–æ–∫
                                    
                                except json.JSONDecodeError as retry_e:
                                    print(f"  ‚ùå JSON –æ—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {restart_attempt+1}: {retry_e}")
                                    if restart_attempt < max_restart_attempts - 1:
                                        print(f"    –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑...")
                                        continue
                                except Exception as retry_e:
                                    print(f"  ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {restart_attempt+1}: {retry_e}")
                                    if restart_attempt < max_restart_attempts - 1:
                                        print(f"    –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑...")
                                        continue
                            else:
                                print(f"  ‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç VLLM –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {restart_attempt+1}")
                                if restart_attempt < max_restart_attempts - 1:
                                    print(f"    –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑...")
                                    continue
                        else:
                            print(f"  ‚ùå –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {restart_attempt+1}")
                            if restart_attempt < max_restart_attempts - 1:
                                print(f"    –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑...")
                                continue
                    
                    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
                    if restart_successful:
                        continue  # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Å–µ–≥–º–µ–Ω—Ç—É
                    else:
                        print(f"  ‚ö†Ô∏è –í—Å–µ {max_restart_attempts} –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å")
                    
                    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ fallback
                    result = {
                        'segment_index': i,
                        'start_time': segment['start'],
                        'end_time': segment['end'],
                        'duration': segment['duration'],
                        'transcript': transcript,
                        'analysis': clean_json,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç LLM –∫–∞–∫ –µ—Å—Ç—å
                        'frame_count': len(base64_frames),
                        'parse_error': str(e),
                        'success': False
                    }
                    results.append(result)
                    print(f"  ‚ö†Ô∏è Segment {i+1} saved as fallback with original LLM response")
                    
                except Exception as e:
                    print(f"  ‚ùå Unexpected error processing segment {i+1}:")
                    print(f"      Error: {e}")
                    import traceback
                    traceback.print_exc()
                    print(f"      Raw VLLM response: {repr(analysis)}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ fallback –¥–∞–∂–µ –ø—Ä–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
                    result = {
                        'segment_index': i,
                        'start_time': segment['start'],
                        'end_time': segment['end'],
                        'duration': segment['duration'],
                        'transcript': transcript,
                        'analysis': analysis,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç
                        'frame_count': len(base64_frames),
                        'error': str(e)
                    }
                    results.append(result)
                    print(f"  ‚ö†Ô∏è Segment {i+1} saved as fallback with raw response")
            else:
                print(f"  ‚ùå VLLM analysis failed for segment {i+1} after {max_retries} attempts")
        
        print(f"\n‚úì Phase 2 batch complete: {len(results)} –Ω–æ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")

        # Save results
        self.save_phase2_results(video_name, results)

        # Update ChromaDB status to "completed"
        self.db_manager.update_video_status(
            video_name=video_name,
            phase2_status="completed",
            phase2_segments_analyzed=len(results)
        )

        print(f"‚úì Phase 2 analysis complete for {video_name}. ChromaDB status updated.")
    
    def load_existing_results(self, video_name: str) -> Optional[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Phase 2"""
        output_path = Path('output') / f'{video_name}_phase2_vllm_analysis.json'
        
        if not output_path.exists():
            print(f"No existing results found: {output_path}")
            return None
            
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úì Loaded existing results: {len(data.get('segments', []))} segments")
                return data
        except Exception as e:
            print(f"‚ùå Error loading existing results: {e}")
            return None
    
    def get_segments_to_process(self, video_name: str, total_segments: int) -> List[int]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å"""
        if FULL_REPROCESS:
            print(f"üîÑ FULL_REPROCESS=True - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ {total_segments} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∑–∞–Ω–æ–≤–æ")
            return list(range(total_segments))
        
        existing = self.load_existing_results(video_name)
        if not existing:
            print(f"üìù –ù–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ {total_segments} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            return list(range(total_segments))
        
        # –ù–∞—Ö–æ–¥–∏–º —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        processed_indices = set()
        for segment in existing.get('segments', []):
            # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª—è success, –Ω–æ –µ—Å—Ç—å analysis –±–µ–∑ –æ—à–∏–±–æ–∫
            is_success = segment.get('success', False) or (
                'analysis' in segment and 
                'parse_error' not in segment and 
                'error' not in segment
            )
            if is_success:
                processed_indices.add(segment.get('segment_index'))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ
        missing = [i for i in range(total_segments) if i not in processed_indices]
        
        print(f"üìä –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"   –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {total_segments}")
        print(f"   –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {len(processed_indices)}")
        print(f"   –ù—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(missing)}")
        if missing:
            print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã: {missing[:10]}{'...' if len(missing) > 10 else ''}")
        
        return missing
    
    def merge_results(self, video_name: str, new_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏"""
        existing = self.load_existing_results(video_name)
        
        if not existing or FULL_REPROCESS:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            all_segments = new_results
        else:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            existing_segments = {seg['segment_index']: seg for seg in existing.get('segments', [])}
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for new_seg in new_results:
                existing_segments[new_seg['segment_index']] = new_seg
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
            all_segments = [existing_segments[i] for i in sorted(existing_segments.keys())]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        success_count = sum(1 for seg in all_segments if seg.get('success', False))
        total_count = len(all_segments)
        expected_total = self.load_phase1_data(video_name)
        expected_count = len(expected_total) if expected_total else 60
        
        analysis_complete = (total_count == expected_count and success_count == expected_count)
        
        return {
            'video_info': {
                'video_name': video_name,
                'total_segments': expected_count,
                'processed_segments': total_count,
                'success_segments': success_count,
                'analysis_complete': analysis_complete,
                'analysis_method': 'phase2_vllm_chunked_50s_overlap_15s',
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'segments': all_segments
        }

    def save_phase2_results(self, video_name: str, results: List[Dict[str, Any]]) -> None:
        """Save Phase 2 VLLM analysis results with merge and backup"""
        if not results:
            print("No results to save")
            return
        
        output_path = Path('output') / f'{video_name}_phase2_vllm_analysis.json'
        backup_path = Path('output') / f'{video_name}_phase2_vllm_analysis.json.bak'
        
        # –°–æ–∑–¥–∞—Ç—å backup —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞
        if output_path.exists():
            try:
                shutil.copy2(output_path, backup_path)
                print(f"‚úì Backup created: {backup_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to create backup: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–º–µ—Ç–∫–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∫ –Ω–æ–≤—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        for result in results:
            # –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º –æ—à–∏–±–æ–∫
            result['success'] = not ('parse_error' in result or 'error' in result)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        merged_data = self.merge_results(video_name, results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(merged_data, f, ensure_ascii=False, indent=2)
            
            print(f"\n‚úì Phase 2 results saved to: {output_path}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            video_info = merged_data['video_info']
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {video_info['total_segments']}")
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {video_info['processed_segments']}")
            print(f"   –£—Å–ø–µ—à–Ω–æ: {video_info['success_segments']}")
            print(f"   –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {'‚úÖ –î–ê' if video_info['analysis_complete'] else '‚ùå –ù–ï–¢'}")
            
        except Exception as e:
            print(f"‚ùå Error saving results: {e}")
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ backup –µ—Å–ª–∏ –µ—Å—Ç—å
            if backup_path.exists():
                try:
                    shutil.copy2(backup_path, output_path)
                    print(f"‚úì Restored from backup")
                except:
                    pass
        
        # Show sample results 
        print("\nSample VLLM analysis results:")
        for i, result in enumerate(results[:3]):
            print(f"\n  Segment {i+1} ({result['start_time']:.1f}s - {result['end_time']:.1f}s):")
            print(f"    Transcript: {result['transcript']}")
            print(f"    Analysis: {result['analysis']}")
        
        if len(results) > 3:
            print(f"\n  ... and {len(results) - 3} more segments")


def test_model_control():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä—è–º–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—å—é"""
    print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—å—é ===")
    
    analyzer = Phase2VLLMAnalyzer()
    
    # –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
    print("\n1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–¥–µ–ª–∏:")
    status = analyzer.get_model_status()
    if status:
        print(f"   –°—Ç–∞—Ç—É—Å: {status.get('status')}")
        print(f"   –ú–æ–¥–µ–ª—å: {status.get('model')}")
        print(f"   PID: {status.get('pid')}")
        print(f"   –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {status.get('uptime_seconds')} —Å–µ–∫")
    else:
        print("   –°—Ç–∞—Ç—É—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    # –¢–µ—Å—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏
    print("\n2. –¢–µ—Å—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏:")
    analyzer.stop_model(reason="test_stop")
    
    # –ñ–¥–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    if analyzer.wait_for_model_stop():
        print("   ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
    else:
        print("   ‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∞—Å—å")
    
    # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ
    time.sleep(5)
    
    # –¢–µ—Å—Ç –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏
    print("\n3. –¢–µ—Å—Ç –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏:")
    analyzer.start_model(reason="test_start")
    
    # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
    if analyzer.wait_for_model_restart():
        print("   ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–∞")
    else:
        print("   ‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª–∞—Å—å")
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
    print("\n4. –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å:")
    status = analyzer.get_model_status()
    if status:
        print(f"   –°—Ç–∞—Ç—É—Å: {status.get('status')}")
        print(f"   –ú–æ–¥–µ–ª—å: {status.get('model')}")
        print(f"   PID: {status.get('pid')}")
    else:
        print("   –°—Ç–∞—Ç—É—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    # –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    print("\n5. –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API:")
    if analyzer.test_vllm_connection():
        print("   ‚úÖ API –¥–æ—Å—Ç—É–ø–µ–Ω")
    else:
        print("   ‚ùå API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ===")


def main():
    """Main function for Phase 2"""
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--test-control":
        test_model_control()
        return

    print("=== Phase 2: VLLM Analysis ===")

    analyzer = Phase2VLLMAnalyzer()

    # Get videos from ChromaDB where phase1 completed and phase2 pending/processing
    videos_to_process = analyzer.db_manager.list_videos(
        status_filter={
            "phase1_status": "completed",
            "phase2_status": ["pending", "processing"]
        }
    )

    if not videos_to_process:
        print("‚úÖ No videos to process - all Phase 2 complete or Phase 1 not ready!")
        return

    print(f"\nüìã Found {len(videos_to_process)} videos to process:")
    for video_info in videos_to_process:
        print(f"  - {video_info['video_name']}: phase2_status={video_info['phase2_status']}")

    # Process videos
    for video_info in videos_to_process:
        video_name = video_info["video_name"]

        print(f"\n*** Processing video: {video_name} ***")

        # Check if Phase 1 data exists
        phase1_file = Path('output') / f'{video_name}_phase1_data.pkl'
        if not phase1_file.exists():
            print(f"‚ùå Phase 1 data not found: {phase1_file}")
            continue

        analyzer.process_phase2_analysis(video_name)
        print(f"‚úÖ Phase 2 completed for: {video_name}")

    print("\n=== Phase 2: All videos analyzed ===")


if __name__ == "__main__":
    main()